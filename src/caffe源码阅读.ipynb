{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# caffe源码阅读\n",
    "非原创\n",
    "\n",
    "[TOC]\n",
    "\n",
    "## 1. 框架层次\n",
    "### 1. Blob\n",
    "SynceMemory的一种包装，是caffe中Layer， Net， 以及Solver之间数据交互的基本计算单元\n",
    "\n",
    "### 2. Filter\n",
    "Filter，继承Layer, 用随机产生的数值填充Blob\n",
    "\n",
    "BilinearFiller，继承Filler， 用于通过双线性插值的方式填充Blob\n",
    "\n",
    "ConstantFiller，继承Filler， 用于通过常数填充Blob\n",
    "\n",
    "GaussianFiller，继承Filler， 用于通过高斯分布填充Blob\n",
    "\n",
    "MSRAFiller，继承Filler， 用于通过$x\\sim N(0, \\sigma^2)$分布填充Blob\n",
    "\n",
    "PositiveUnitballFiller，继承Filler， 用以下方式填充Blob，$x\\in [0, 1], \\forall i\\sum_{j}x_{ij}=1$\n",
    "\n",
    "UniformFiller，继承Filler， 用指定范围的均匀分布填充Blob\n",
    "\n",
    "XavierFiller，继承Filler， 用$x\\sim U(-a, +a)$其中a的值为输出输出节点数的比值的倒数或平均\n",
    "\n",
    "### 3. Layer\n",
    "#### 1. 数据输入层\n",
    "BasePreFetchingDataLayer\n",
    "\n",
    "BasePreFetchingDataLayer\n",
    "\n",
    "MemoryDataLayer, 继承BaseDataLayer \n",
    "DataLayer, 继承BasePreFetchingDataLayer\n",
    "\n",
    "ImageDataLayer, 继承BasePreFetchingDataLayer\n",
    "\n",
    "WindowDataLayer, 继承BasePreFetchingDataLayer\n",
    "\n",
    "HDF5DataLayer,  继承Layer, 用于向Net提供HDF5格式的数据\n",
    "\n",
    "LevelDBCursor\n",
    "\n",
    "LMDBCursor\n",
    "\n",
    "DummyDataLayer, 继承Layer, 用于向Net提供Filler生成的数据\n",
    "\n",
    "InputLayer, 继承Layer, 用于向Net提供直接直接赋值的数据\n",
    "\n",
    "HDF5OutputLayer, 继承LossLayer，用于将Blob写入到hdf5格式的文件中\n",
    "\n",
    "DataTransformer，用于在输入数据中加入通用的变换，比如尺度、镜像以及减均值等操作。\n",
    "\n",
    "#### 2. 激活层\n",
    "NeuronLayer, 继承LossLayer， 用于提供一种接口，输入一个Blob，进行元素级的操作后输出一个大小相同的Blob\n",
    "\n",
    "AbsValLayer, 继承NeuronLayer， 进行绝对值操作\n",
    "\n",
    "BNLLLayer, 继承NeuronLayer，$y=x+log(1+exp(-x)) ,if x>0; y=log(1+exp(x)), otherwise$\n",
    "\n",
    "DropoutLayer, 继承NeuronLayer， 用于在训练时候按指定概率随机设置一些元素的值为0\n",
    "\n",
    "ELULayer, 继承NeuronLayer， Exponential Linear Unit, $y=x, if x>0; y = \\alpha(exp(x)-1) ,otherwise$\n",
    "\n",
    "ExpLayer, 继承NeuronLayer， 用于计算$y=\\gamma^{\\alpha x + \\beta}$\n",
    "\n",
    "LogLayer, 继承NeuronLayer, 用于计算$y=log_{\\gamma}(\\alpha x + \\beta)$\n",
    "\n",
    "PowerLayer, 继承NeuronLayer， 用于计算$y=(\\alpha x + \\beta)^{\\gamma}$\n",
    "\n",
    "PReLULayer, 继承NeuronLayer, Parameterized Rectified Linear Unit， $y_i=max(0, x_i)+a_{i}min(0, x_i)$\n",
    "\n",
    "ReLULayer, 继承NeuronLayer, Rectified Linear Unit non-linearity 整流线性单元，$y=max(x, 0)$\n",
    "\n",
    "SigmoidLayer, 继承NeuronLayer,Sigmoid function S型曲线,  $y=(1+exp(-x))^{-1}$\n",
    "\n",
    "TanHLayer, 继承NeuronLayer, TanH hyperbolic tangent non-linearity 双曲正切, $y=\\frac{exp(2x)-1}{exp(2x)+1}$\n",
    "\n",
    "ThresholdLayer, 继承NeuronLayer, 高于阈值输出1，否则输出0\n",
    "\n",
    "#### 3. 视觉层\n",
    "BaseConvolutionLayer\n",
    "\n",
    "DeconvolutionLayer\n",
    "\n",
    "ConvolutionLayer\n",
    "\n",
    "PoolingLayer, 继承LossLayer，在局部区域内进行取最大值或者平均值的操作\n",
    "\n",
    "LRNLayer, 继承LossLayer，计算局部响应归一化\n",
    "\n",
    "Im2colLayer, 继承LossLayer，用于将图像重新排列为向量的操作\n",
    "\n",
    "#### 4. 通用计算层\n",
    "AccuracyLayer, 继承Layer, 用于计算单类别分类精度\n",
    "\n",
    "ArgMaxLayer, 继承Layer, 用于获取指定维度上第k大的值的索引\n",
    "\n",
    "BatchNormLayer, 继承Layer, 用于将一个batch输入归一化为0均值，1标准差\n",
    "\n",
    "BatchReindexLayer, 继承Layer, 用于在一个batch内选择，重新排列或者重复样本， 传入的第二个blob为int类型，并用于索引第一个Blob的第一个维度\n",
    "\n",
    "BiasLayer, 继承Layer, 用于计算两个Blob输入的和\n",
    "\n",
    "ConcatLayer, 继承Layer, 用于将两个或两个以上的Blob输入在指定的维度上拼接在一起\n",
    "\n",
    "CropLayer, 继承Layer, 用于从第一个Blob输入中截取第二个Blob输入所指定的形状区域\n",
    "\n",
    "EltwiseLayer, 继承Layer, 用于进行多个输入Blob之间的元素级的乘法或加法操作\n",
    "\n",
    "EmbedLayer, 继承Layer, 自动将类别索引转换为one-hot格式的向量，然后进行全连接的操作\n",
    "\n",
    "FilterLayer, 继承Layer, 接收两个输入的Blob，其中后一个Blob作为前一个Blob的选择器，0表示过滤，非零表示保留\n",
    "\n",
    "FlattenLayer, 继承Layer, 用于将输入Blob转换为向量形式的Blob\n",
    "\n",
    "InnerProductLayer, 继承Layer, 用于全连接层的构建，其中的偏移参数为可选的部分\n",
    "\n",
    "LSTMUnitLayer, 继承Layer, 用于进行单步的LSTM单元的构建\n",
    "\n",
    "MVNLayer, 继承Layer, 用于将输入归一化为均值为0， 方差为1\n",
    "\n",
    "ParameterLayer, 继承Layer, \n",
    "\n",
    "PythonLayer, 继承Layer, \n",
    "\n",
    "RecurrentLayer, 继承Layer,  一个用于封装循环网络的抽象类\n",
    "\n",
    "LSTMLayer, 继承RecurrentLayer，构建基本单元为LSTM的循环网络\n",
    "\n",
    "RNNLayer,  继承RecurrentLayer，构建简单的循环网络\n",
    "\n",
    "ReductionLayer, 继承Layer, 用于从任意形状输入Blob根据指定操作(和，绝对和，平方和)产生标量输出\n",
    "\n",
    "ReshapeLayer, 继承Layer, \n",
    "\n",
    "ScaleLayer, 继承Layer, 用于计算元素级的乘法，第二个参数采用广播的方式匹配第一个参数的大小\n",
    "\n",
    "SilenceLayer, 继承Layer, 用于在不产生输出Blob的层中忽略输入\n",
    "\n",
    "SliceLayer, 继承Layer, 用于进行切片操作\n",
    "\n",
    "SoftmaxLayer, 继承Layer, 计算Softmax\n",
    "\n",
    "SplitLayer, 继承Layer, 用于将单个输入复制分出多个输出分支\n",
    "\n",
    "SPPLayer, 继承Layer, 利用最大或平均操作进行空间金字塔池化\n",
    "\n",
    "TileLayer, 继承Layer, 用于在指定维度上复制输出Blob\n",
    "\n",
    "#### 5. 损失函数层\n",
    "LossLayer, 继承Layer,作为一种接口 用于根据一个预测的Blob和一个真值的Blob计算损失\n",
    "\n",
    "ContrastiveLossLayer, 继承LossLayer， 用于计算Contrastive损失 $E=\\frac{1}{N}\\sum_{n=1}^{N}(y)d^2 + (1-y)max(margin-d, 0)^2$\n",
    "\n",
    "EuclideanLossLayer, 继承LossLayer，用于计算L2损失， $E=\\frac{1}{N}\\sum_{n=1}^{N}||\\hat{y}_n-y_n||^2$\n",
    "\n",
    "HingeLossLayer, 继承LossLayer，计算单类别的Hinge损失\n",
    "\n",
    "InfogainLossLayer, 继承LossLayer，计算信息增益损失\n",
    "\n",
    "MultinomialLogisticLossLayer, 继承LossLayer，计算多元逻辑损失\n",
    "\n",
    "SigmoidCrossEntropyLossLayer, 计算交叉熵损失， $E=-\\frac{1}{N}\\sum_{n=1}^{N}[p_nlog(\\hat{p}_n)+(1-p_n)log(1-\\hat{p}_n)]$\n",
    "\n",
    "SoftmaxWithLossLayer, 继承LossLayer，计算单类别的多元逻辑损失\n",
    "\n",
    "### 4. Net\n",
    "将多个层加入到一个有向无环图中形成网络。\n",
    "\n",
    "### 5. Solver\n",
    "SGDSolver, 继承Solver，随机梯度下降优化器\n",
    "\n",
    "AdaDeltaSolver, 继承SGDSolver\n",
    "\n",
    "AdaGradSolver, 继承SGDSolver\n",
    "\n",
    "AdamSolver, 继承SGDSolver，可以自适应调节学习率的SGD优化器\n",
    "\n",
    "NesterovSolver, 继承SGDSolver\n",
    "\n",
    "RMSPropSolver, 继承SGDSolver\n",
    "\n",
    "## 2. 常用Protocol文件的配置\n",
    "### 1. 数据层\n",
    "#### 从LevelDB或者LMDB的数据库文件获取数据\n",
    "```protobuf\n",
    "layer {\n",
    " name: \"cifar\"              # 表示该层的名字\n",
    " type: \"Data\"               # 设置层的类型， Data表示来源为LevelDB或LMDB \n",
    " top: \"data\"                # 输出数据\n",
    " top: \"label\"               # 输出数据\n",
    " include {\n",
    "   phase: TRAIN             # 设置该层的作用范围为训练\n",
    "  }\n",
    " transform_param {          # 用于对数据进行预处理操作，下面三个处理是可选的\n",
    "   mean_file: \"examples/cifar10/mean.binaryproto\"   # 用于进行减均值操作\n",
    "   mirror: 1                # 1表示开启镜像，0表示关闭，也可用ture和false来表示\n",
    "   crop_size: 227           # 剪裁一个 227*227的图块，在训练阶段随机剪裁，在测试阶段从中间裁剪\n",
    "  }\n",
    " data_param {               # 设置数据读取参数\n",
    "   source: \"examples/cifar10/cifar10_train_lmdb\"    # 设置数据源的文件\n",
    "   batch_size: 100          # 设置batchSize\n",
    "   backend: LMDB            # 设置数据的后端数据库类型，默认为LevelDB\n",
    "  }\n",
    "}\n",
    "```\n",
    "#### 从内存读取中获取数据\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"memory_data\"\n",
    "  type: \"MemoryData\"        # 设置层类型，MemoryData为从内存获取数据\n",
    "  top: \"data\"\n",
    "  top: \"label\"\n",
    "  memory_data_param{\n",
    "    batch_size: 2\n",
    "    height: 100\n",
    "    width: 100\n",
    "    channels: 1\n",
    "  }\n",
    "  transform_param {\n",
    "    scale: 0.0078125\n",
    "    mean_file: \"mean.proto\"\n",
    "    mirror: false\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 从HDF5文件获取数据\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"data\"\n",
    "  type: \"HDF5Data\"          # 设置层类型，HDF5Data为从HDF5文件获取数据\n",
    "  top: \"data\"\n",
    "  top: \"label\"\n",
    "  hdf5_data_param {\n",
    "    source:\"examples/hdf5_classification/data/train.txt\"\n",
    "    batch_size: 10\n",
    "  }\n",
    "}\n",
    "```\n",
    "#### 从图片获取数据\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"data\"\n",
    "  type: \"ImageData\"         # 设置层类型，ImageData为从HDF5文件获取数据\n",
    "  top: \"data\"\n",
    "  top: \"label\"\n",
    "  transform_param {\n",
    "    mirror: false\n",
    "    crop_size: 227\n",
    "    mean_file:\"data/ilsvrc12/imagenet_mean.binaryproto\"\n",
    "  }\n",
    "  image_data_param {\n",
    "    source: \"examples/_temp/file_list.txt\"\n",
    "    batch_size: 50\n",
    "    new_height: 256\n",
    "    new_width: 256\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 从窗口获取数据\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"data\"\n",
    "  type: \"WindowData\"        # 设置层类型，WindowData为从窗口获取数据\n",
    "  top: \"data\"\n",
    "  top: \"label\"\n",
    "  include {\n",
    "    phase: TRAIN\n",
    "  }\n",
    "  transform_param {\n",
    "    mirror: true\n",
    "    crop_size: 227\n",
    "    mean_file:\"data/ilsvrc12/imagenet_mean.binaryproto\"\n",
    "  }\n",
    "  window_data_param{\n",
    "    source:\"examples/finetune_pascal_detection/window_file_2007_trainval.txt\"\n",
    "    batch_size: 128\n",
    "    fg_threshold: 0.5\n",
    "    bg_threshold: 0.5\n",
    "    fg_fraction: 0.25\n",
    "    context_pad: 16\n",
    "    crop_mode: \"warp\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.视觉层\n",
    "#### 卷积层\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"data\"\n",
    "  top: \"conv1\"\n",
    "  param {\n",
    "    lr_mult: 1             # 权重学习率乘数，与base_lr相乘\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2             # 偏移学习率乘数，与base_lr相乘\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 20         # 输出通道数\n",
    "    kernel_size: 5         # 卷积核的大小\n",
    "    stride: 1              # 卷积核滑动的步幅\n",
    "    pad: 2                 \n",
    "# 扩充边缘，默认为0，不扩充。扩充的时候是左右、上下对称的，比如卷积核的大小为5*5，那么pad设置为2，则四个边缘都扩充2个像素，即宽度和高度都扩充了4个像素,这样卷积运算之后的特征图就不会变小。也可以通过pad_h和pad_w来分别设定。\n",
    "    weight_filler {        # 权重的初始化填充方式设置\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {          # 偏移的初始化填充方式设置\n",
    "      type: \"constant\"\n",
    "    }\n",
    "     bias_term: true       # 是否开启偏置项，默认为true,开启\n",
    "      group: 1             \n",
    "# 分组，默认为1组。如果大于1，我们限制卷积的连接操作在一个子集内。如果我们根据图像的通道来分组，那么第i个输出分组只能与第i个输入分组进行连接。\n",
    "  }\n",
    "}\n",
    "# 输出特征图的大小\n",
    "# w1=(w0+2*pad-kernel_size)/stride+1;\n",
    "# h1=(h0+2*pad-kernel_size)/stride+1;\n",
    "# 如果设置stride为1，前后两次卷积部分存在重叠。如果设置pad=(kernel_size-1)/2,则运算后，宽度和高度不变。\n",
    "```\n",
    "\n",
    "#### 池化层\n",
    "\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"pool1\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"pool1\"\n",
    "  pooling_param {\n",
    "    pool: MAX   # 池化方法，默认为MAX。目前可用的方法有MAX, AVE, 或STOCHASTIC\n",
    "    kernel_size: 3         # 池化的核大小。也可以用kernel_h和kernel_w分别设定\n",
    "    stride: 2              # 池化核滑动的步幅， 默认为1\n",
    "    pad: 1                 # 和卷积层的pad的一样，进行边缘扩充。默认为0\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 局部响应归一化层\n",
    "```protobuf\n",
    "layers {\n",
    "  name: \"norm1\"\n",
    "  type: LRN\n",
    "  bottom: \"pool1\"\n",
    "  top: \"norm1\"\n",
    "  lrn_param {\n",
    "# 默认为5。如果是跨通道LRN，则表示求和的通道数；如果是在通道内LRN，则表示求和的正方形区域长度。\n",
    "    local_size: 5\n",
    "    alpha: 0.0001         # 默认为1，归一化公式中的参数。\n",
    "    beta: 0.75            # 默认为5，归一化公式中的参数。\n",
    "    norm_region: ACROSS_CHANNELS \n",
    "# 默认为ACROSS_CHANNELS。有两个选择，ACROSS_CHANNELS表示在相邻的通道间求和归一化。WITHIN_CHANNEL表示在一个通道内部特定的区域内进行求和归一化。与前面的local_size参数对应。\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### im2col层\n",
    "在caffe中，卷积运算就是先对数据进行im2col操作，再进行内积运算（inner product)。这样做，比原始的卷积操作速度更快。\n",
    "\n",
    "### 3. 激活层\n",
    "#### Sigmoid\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"encode1neuron\"\n",
    "  bottom: \"encode1\"\n",
    "  top: \"encode1neuron\"\n",
    "  type: \"Sigmoid\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### ReLU\n",
    "```protobuf\n",
    "layer {\n",
    " name: \"relu1\"\n",
    " type: \"ReLU\"\n",
    " bottom: \"pool1\"\n",
    " top: \"pool1\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### TanH\n",
    "```protobuf\n",
    "layer {\n",
    " name: \"layer\"\n",
    " type: \"TanH\"\n",
    " bottom: \"in\"\n",
    " top: \"out\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### AbsVal\n",
    "```protobuf\n",
    "layer {\n",
    " name: \"layer\"\n",
    " bottom: \"in\"\n",
    " top: \"out\"\n",
    " type: \"AbsVal\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Power\n",
    "```protobuf\n",
    "layer {\n",
    " name: \"layer\"\n",
    " bottom: \"in\"\n",
    " top: \"out\"\n",
    " type: \"Power\"\n",
    " power_param {            # 可选参数\n",
    "   power: 2               # power: 默认为1\n",
    "   scale: 1               # scale: 默认为1\n",
    "   shift: 0               # shift: 默认为0\n",
    " }\n",
    "}\n",
    "```\n",
    "\n",
    "#### BNLL (Binomial normal log likelihood)\n",
    "```protobuf\n",
    "layer {\n",
    " name: \"layer\"\n",
    " type: “BNLL”\n",
    " bottom: \"in\"\n",
    " top: \"out\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. 其他常用层\n",
    "#### Softmax-loss\n",
    "输出损失值\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"loss\"\n",
    "  type: \"SoftmaxWithLoss\"\n",
    "  bottom: \"ip1\"\n",
    "  bottom: \"label\"\n",
    "  top: \"loss\"\n",
    "}\n",
    "```\n",
    "#### Softmax\n",
    "输出似然值，集概率分布\n",
    "```protobuf\n",
    "layers {\n",
    "  bottom: \"cls3_fc\"\n",
    "  top: \"prob\"\n",
    "  name: \"prob\"\n",
    "  type: “Softmax\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Inner Product\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"ip1\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool2\"\n",
    "  top: \"ip1\"\n",
    "  param {\n",
    "    lr_mult: 1                # 权重学习率乘数，与base_lr相乘\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2                # 偏移学习率乘数，与base_lr相乘\n",
    "  }\n",
    "  inner_product_param {\n",
    "    num_output: 500           # 输出单元个数\n",
    "    weight_filler {\n",
    "      type: \"xavier\"\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Accuracy\n",
    "```protobuf\n",
    "layer {\n",
    "  name: \"accuracy\"\n",
    "  type: \"Accuracy\"\n",
    "  bottom: \"ip2\"\n",
    "  bottom: \"label\"\n",
    "  top: \"accuracy\"\n",
    "  include {\n",
    "    phase: TEST                # 只有test阶段才有，因此需要加入include参数\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Reshape\n",
    "```protobuf\n",
    "layer {\n",
    "    name: \"reshape\"\n",
    "    type: \"Reshape\"\n",
    "    bottom: \"input\"\n",
    "    top: \"output\"\n",
    "    reshape_param {\n",
    "      shape {\n",
    "        dim: 0                  # 0表示维度不变，即输入和输出是相同的维度\n",
    "        dim: 2                  # 将原来的维度变换为2\n",
    "        dim: 3                  # 将原来的维度变换为3\n",
    "        dim: -1                 # 自动推理计算出当前维度\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "#### Dropout\n",
    "```protobuf\n",
    "layer {\n",
    " name: \"drop7\"\n",
    " type: \"Dropout\"\n",
    " bottom: \"fc7-conv\"\n",
    " top: \"fc7-conv\"\n",
    " dropout_param {\n",
    "   dropout_ratio: 0.5        # 设置dropout的概率\n",
    " }\n",
    "}\n",
    "```\n",
    "\n",
    "### 5.  Net的配置文件\n",
    "```protobuf\n",
    "name: \"LogReg\"              # 设置网络模型的名字\n",
    "layer {\n",
    " name: \"mnist\"\n",
    " type: \"Data\"\n",
    " top: \"data\"\n",
    " top: \"label\"\n",
    " data_param {\n",
    "   source: \"input_leveldb\"\n",
    "   batch_size: 64\n",
    " }\n",
    "}\n",
    "layer {\n",
    " name: \"ip\"\n",
    " type: \"InnerProduct\"\n",
    " bottom: \"data\"\n",
    " top: \"ip\"\n",
    " inner_product_param {\n",
    "   num_output: 2\n",
    " }\n",
    "}\n",
    "layer {\n",
    " name: \"loss\"\n",
    " type: \"SoftmaxWithLoss\"\n",
    " bottom: \"ip\"\n",
    " bottom: \"label\"\n",
    " top: \"loss\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 6. Solver的配置文件\n",
    "```protobuf\n",
    "# 网络模型文件路径, 也可进行如下配置\n",
    "# train_net: path/to/training/net\n",
    "# test_net: path/to/testing/net\n",
    "net:\"examples/mnist/lenet_train_test.prototxt\"\n",
    "test_iter:100    # 总的测试样本除以batchsize\n",
    "test_interval:500\t# 一个epoch进行一次， 总的训练样本除以batchsize\n",
    "base_lr:0.01        # 设置学习率\n",
    "momentum:0.9        # 上一次梯度更新的权重\n",
    "type:SGD            # 选择优化算法，默认SGD\n",
    "weight_decay:0.0005 # 权重衰减项，防止过拟合\n",
    "lr_policy:\"inv\"     # 见下文\n",
    "gamma:0.0001        # lr_policy=inv时的参数\n",
    "power:0.75          # lr_policy=inv时的参数\n",
    "display:100         # 每训练100次，在屏幕上显示一次\n",
    "max_iter:20000      # 最大迭代次数\n",
    "snapshot:5000       # 没训练5000次保存一次快照\n",
    "snapshot_prefix:\"examples/mnist/lenet\"  # 设置快照前缀\n",
    "solver_mode:CPU     # 设置求解器的运行模式为CPU\n",
    "```\n",
    "\n",
    "lr_policy可以设置为下面这些值，相应的学习率的计算为：\n",
    "- fixed:保持base_lr不变.\n",
    "- step: 如果设置为step,则还需要设置一个stepsize,  返回\n",
    "      base_lr * gamma ^ (floor(iter /stepsize)),其中iter表示当前的迭代次数\n",
    "- exp: 返回base_lr * gamma ^ iter， iter为当前迭代次数\n",
    "- inv:如果设置为inv,还需要设置一个power, 返回base_lr * (1 + gamma * iter) ^ (- power)\n",
    "  -multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据 stepvalue值变化\n",
    "- poly:学习率进行多项式误差, 返回 base_lr (1 -iter/max_iter) ^ (power)\n",
    "  -sigmoid:学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))\n",
    "  multistep示例：\n",
    "```protobuf\n",
    "base_lr:0.01\n",
    "momentum:0.9\n",
    "weight_decay:0.0005\n",
    "# Thelearning rate policy\n",
    "lr_policy:\"multistep\"\n",
    "gamma:0.9\n",
    "stepvalue:5000\n",
    "stepvalue:7000\n",
    "stepvalue:8000\n",
    "stepvalue:9000\n",
    "stepvalue:9500\n",
    "```\n",
    "\n",
    "优化器有6种：\n",
    "#### StochasticGradient Descent (type: \"SGD\"),\n",
    "```protobuf\n",
    "base_lr: 0.01\n",
    "lr_policy: \"step\"\n",
    "gamma: 0.1  \n",
    "stepsize: 1000 \n",
    "max_iter: 3500\n",
    "momentum: 0.9\n",
    "```\n",
    "#### AdaDelta(type: \"AdaDelta\"),\n",
    "```protobuf\n",
    "net: \"examples/mnist/lenet_train_test.prototxt\"\n",
    "test_iter: 100\n",
    "test_interval: 500\n",
    "base_lr: 1.0\n",
    "lr_policy: \"fixed\"\n",
    "momentum: 0.95\n",
    "weight_decay: 0.0005\n",
    "display: 100\n",
    "max_iter: 10000\n",
    "snapshot: 5000\n",
    "snapshot_prefix: \"examples/mnist/lenet_adadelta\"\n",
    "solver_mode: GPU\n",
    "type: \"AdaDelta\"\n",
    "delta: 1e-6\n",
    "```\n",
    "#### AdaptiveGradient (type: \"AdaGrad\"),\n",
    "```protobuf\n",
    "net:\"examples/mnist/mnist_autoencoder.prototxt\"\n",
    "test_state: { stage: 'test-on-train' }\n",
    "test_iter: 500\n",
    "test_state: { stage: 'test-on-test' }\n",
    "test_iter: 100\n",
    "test_interval: 500\n",
    "test_compute_loss: true\n",
    "base_lr: 0.01\n",
    "lr_policy: \"fixed\"\n",
    "display: 100\n",
    "max_iter: 65000\n",
    "weight_decay: 0.0005\n",
    "snapshot: 10000\n",
    "snapshot_prefix:\"examples/mnist/mnist_autoencoder_adagrad_train\"\n",
    "# solver mode: CPU or GPU\n",
    "solver_mode: GPU\n",
    "type: \"AdaGrad\"\n",
    "```\n",
    "#### Adam(type: \"Adam\"),\n",
    "```protobuf\n",
    "net: \"examples/mnist/lenet_train_test.prototxt\"  \n",
    "test_iter: 100  \n",
    "test_interval: 500  \n",
    "#All parameters are from the cited paper above  \n",
    "base_lr: 0.001  \n",
    "momentum: 0.9  \n",
    "momentum2: 0.999  \n",
    "#since Adam dynamically changes the learning rate, we set the base learning  \n",
    "#rate to a fixed value  \n",
    "lr_policy: \"fixed\"  \n",
    "display: 100  \n",
    "#The maximum number of iterations  \n",
    "max_iter: 10000  \n",
    "snapshot: 5000  \n",
    "snapshot_prefix: \"examples/mnist/lenet\"  \n",
    "type: \"Adam\"  \n",
    "solver_mode: GPU \n",
    "```\n",
    "#### Nesterov’s Accelerated Gradient (type: \"Nesterov\")\n",
    "```protobuf\n",
    "net:\"examples/mnist/mnist_autoencoder.prototxt\"\n",
    "test_state: { stage: 'test-on-train' }\n",
    "test_iter: 500\n",
    "test_state: { stage: 'test-on-test' }\n",
    "test_iter: 100\n",
    "test_interval: 500\n",
    "test_compute_loss: true\n",
    "base_lr: 0.01\n",
    "lr_policy: \"step\"\n",
    "gamma: 0.1\n",
    "stepsize: 10000\n",
    "display: 100\n",
    "max_iter: 65000\n",
    "weight_decay: 0.0005\n",
    "snapshot: 10000\n",
    "snapshot_prefix: \"examples/mnist/mnist_autoencoder_nesterov_train\"\n",
    "momentum: 0.95\n",
    "# solver mode: CPU or GPU\n",
    "solver_mode: GPU\n",
    "type: \"Nesterov\"\n",
    "```\n",
    "#### RMSprop(type: \"RMSProp\")\n",
    "```protobuf\n",
    "net:\"examples/mnist/lenet_train_test.prototxt\"\n",
    "test_iter: 100\n",
    "test_interval: 500\n",
    "base_lr: 1.0\n",
    "lr_policy: \"fixed\"\n",
    "momentum: 0.95\n",
    "weight_decay: 0.0005\n",
    "display: 100\n",
    "max_iter: 10000\n",
    "snapshot: 5000\n",
    "snapshot_prefix:\"examples/mnist/lenet_adadelta\"\n",
    "solver_mode: GPU\n",
    "type: \"RMSProp\"\n",
    "rms_decay: 0.98\n",
    "```\n",
    "\n",
    "## 3. 实践练习\n",
    "### 1. 原版Caffe直接运行cifar10的数据集\n",
    "运行 cifar10_full_sigmoid_solver_bn.prototxt， 可以得到在测试集上的精度为67.89%。\n",
    "\n",
    "运行 cifar10_full_solver.prototxt， 可以得到79.25%。\n",
    "\n",
    "运行 cifar10_full_sigmoid_solver.prototxt，可以得到54.29%。\n",
    "\n",
    "运行 cifar10_quick_solver.prototxt，可以得到71.61%。\n",
    "\n",
    "运行 cifar10_quick_solver_lr1.prototxt，可以得到66.89%。\n",
    "\n",
    "运行 train_quick.sh，可以得到75.33%。\n",
    "\n",
    "### 2. 通过Python添加新的Layer\n",
    "\n",
    "\n",
    "### 3. 通过C++添加新的Layer\n",
    "\n",
    "\n",
    "### 4. 将caffe作为第三方库调用\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
